{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irA/CMSC320-Assignment-1-Dr.Alam/blob/main/CMSC320_HW1_Fall2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HOMEWORK 1: CAUTION! CONTENTS ARE HOT** üåã\n",
        "## **DUE: *SEPTEMBER 19, 2024 @ 11:59 PM***\n",
        "## **24-HR LATE DUE DATE WITH A 15% PENALTY: *SEPTEMBER 20, 2024 @ 11:59 PM***\n",
        "\n",
        "---------------------\n",
        "#### **DATASET DESCRIPTION**\n",
        "\n",
        "The [NCEI/WDS Global Significant Volcanic Eruptions Database](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.mgg.hazards:G10147) is a very comprehensive collection of +600 volcanic eruptions dating from 4360 BC to the present. Due to the nature of this assignment, we will be dealing with relatively newer volcanoes (in which some are still obviously still older than anyone on Earth currently). Each eruption in the database is classified as significant if it meets one or more criteria, such as causing fatalities, incurring **damage on property** (**+$1 million**), reaching a **Volcanic Explosivity Index (VEI)** of **6 or higher**, generating a tsunami, or being linked to a significant earthquake. The VEI is a scale that measures the explosiveness of volcanic eruptions, providing insight into the magnitude and potential consequences of the eruptions. The database includes detailed information on the location, type of volcano, last known eruption, VEI, casualties, property damage, and much more.\n",
        "![volcano](https://wikitravel.org/upload/shared//9/99/Volcano_de_Fuego_Banner.jpg)\n",
        "\n",
        "#### **Objective of the Assignment:**\n",
        "\n",
        "**We are going to dive straight into these volcanoes (well... their dataset), to swim our way into Pandas and SQL proficiency!**\n",
        "\n",
        "You will find the [Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html) helpful. There are also some helpful links to guide you along the way! Don't get burned üî•\n",
        "\n",
        "---------------------"
      ],
      "metadata": {
        "id": "FwALoygGD617"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DO NOT REMOVE ANY PART OF ANY OF THE QUESTIONS OR YOU LOSE CREDIT**\n",
        "### *No Hardcoding either*  üòã‚ù§Ô∏è‚Äçüî•\n",
        "### **REMEMBER TO SHOW ALL CODE OUTPUT (NO CREDIT OTHERWISE)**"
      ],
      "metadata": {
        "id": "4sQEC_lpHYdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 1: Maintenance ü§© (25 POINTS TOTAL)**\n",
        "First, we're going to familiarize ourselves with the process. As in most languages, Python looks best when its modules are imported first before any other code is written ‚ú®"
      ],
      "metadata": {
        "id": "HrD3qZmJUp6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure these code blocks run properly and that you have properly installed the appropriate modules required.\n",
        "import pandas as pd\n",
        "import requests\n",
        "# import other libraries here\n",
        "\n",
        "# Don't remove this\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "CruzvUOrKEoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may have noticed, there's another library aside from Pandas called \"[requests](https://requests.readthedocs.io/en/latest/).\" **The requests library allows you to send HTTP requests to a server, retrieve the content, and process it at ease.** It's very beginner friendly for those attempting to get into webscraping (super important for collecting and creating datasets). We also recommend looking into [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) (yeah, soup LOL), another wonderful library that can be paired with the requests library for webscraping.\n",
        "\n",
        "As shown below, sometimes specific websites require specific headers in order to process a request to access the data.\n",
        "\n",
        "To check if a request was processed successfully, use the [status_code](https://requests.readthedocs.io/en/latest/api/) function to see if the process returned 200."
      ],
      "metadata": {
        "id": "7nWwbZ4jK5Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API URL and headers in case request gets denied.\n",
        "api_url = \"https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/volcanoes\"\n",
        "\n",
        "headers = {\n",
        "    'accept': '*/*'\n",
        "}"
      ],
      "metadata": {
        "id": "MMxyFjVrKKvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TASK 1.0: Cute Webscraping (5 points)**\n",
        "To make our cute webscraper we need to **create a GET request** using the relevant information given above.\n",
        "\n",
        "This particular dataset NOAA returns data from the API as ***json*** when a user makes a request. The json data has a particular format, so we will extract our needed information only from the field called **items** to make a dataframe.\n",
        "\n",
        "**After properly scraping the data, name this dataframe** ***df***\n",
        "\n",
        "**Save this dataframe into a CSV file named 'volcanoes.csv'**\n",
        "\n",
        "**You won't need to run this more than once**"
      ],
      "metadata": {
        "id": "zwJMQjBmJ5Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pRfxOpACC39n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TASK 1.1: 1 Liner Thingz (3 points)**\n",
        "\n",
        "We need to get an idea of what this dataset is going to look. In order to do that, let's take a look at some of the most [basic things](https://dataanalytics.buffalostate.edu/pandas-cheat-sheet) our dataframe has.\n",
        "\n",
        "**Read the directions carefully and code your answer with only one line of code.**\n",
        "\n",
        "***CAN'T USE LOOPS. DO NOT DISPLAY THE DATAFRAME, JUST YOUR CODE OUTPUT HERE.***"
      ],
      "metadata": {
        "id": "geMtNQeiEo8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.1:** *In one line of code and **using only one function**, show how many **total datapoints and features** there are in the dataframe **together**.*"
      ],
      "metadata": {
        "id": "6Uyhc9VOJdbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "a_PPlwLyJbbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.2:** *In one line of code, list the **names** of all the **features** in the dataframe.*"
      ],
      "metadata": {
        "id": "OoMluxmiESrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "B4XwpjUUETbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't be using some of the data because there is a lot of missing data.\n",
        "\n",
        "**1.1.3:** *In one line of code, create a **new dataframe** called **new_df** that **contains all** the features of the **old** dataframe **except the following**:*\n",
        "\n",
        "volcanoLocationNum, location, latitude, longitude, agent, significant,\tpublish,\teruption,\tstatus, timeErupt, damageAmountOrder, damageAmountOrderTotal, housesDestroyedAmountOrder,\thousesDestroyedAmountOrderTotal, housesDestroyed,\thousesDestroyedTotal,\tmissingAmountOrder,\tmissingAmountOrderTotal,\tmissing,\tmissingTotal, damageMillionsDollars, damageMillionsDollarsTotal, injuries, injuriesAmountOrder, injuriesTotal, injuriesAmountOrderTotal, deathsAmountOrderTotal, and deathsAmountOrder.\n",
        "\n"
      ],
      "metadata": {
        "id": "6uvL0VpKNeAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "new_df"
      ],
      "metadata": {
        "id": "rmFy8xLFLRrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TASK 1.2: 1 Liner Shenaniganz (7 points)**\n",
        "\n",
        "We're going to tidy up the **new dataframe** a little more with some more advanced 1 liner code.\n",
        "\n",
        "**Read the directions carefully and code your answer with only one line of code.**\n",
        "\n",
        "**For this section, keep the method of display that is already in the box. Write your code as indicated.**\n",
        "\n",
        "***YOU CAN'T USE ONE LINE LOOPS OR ANY KIND OF LOOP.***"
      ],
      "metadata": {
        "id": "c8xP6O37UQp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2.1:** *In one line of code, **discard any row** that contains **NaN** in **any one** of the columns indicating **time**.*"
      ],
      "metadata": {
        "id": "I_SHOaZRIblA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "new_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B9qVE-OdGysV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2.2:** *In one line of code, **reset** the **index column** of the dataframe so that it has **1-based indexing**.*"
      ],
      "metadata": {
        "id": "OO-A8f3gGEpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "new_df"
      ],
      "metadata": {
        "id": "gBBUKUJVFI0Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **deathsTotal** and **deaths**  columns have approximations of the same data with alternating NaNs in each.\n",
        "\n",
        "**1.2.3:** *In one line of code, make a **new column** called **'totalDeaths'** that takes the **max** of the values given between those* ***two*** *columns. If there is* ***NaN*** *in* ***one column*** *and a* ***numerical*** *value in the* ***other***, *it will ***take the numerical value***. ***Only*** if there are* ***NaNs*** *in* ***both*** *columns, the* ***new column will have NaN.***"
      ],
      "metadata": {
        "id": "oSxQRt-xio1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "new_df"
      ],
      "metadata": {
        "id": "GEZ0YdSPdYyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TASK 1.3: Tailoring Time (10 Points)**\n",
        "It's pretty obvious that the year, month, and day look pretty weird in the dataset. We're going to have to do some hardcore cleaning on the [time](https://pandas.pydata.org/docs/user_guide/timeseries.html). We will learn more about data cleaning in class soon, but here we will perform some basic data cleaning.\n",
        "\n",
        "**We need to have only ONE column called** \"***date***\" **that contains the full date (YYYY-MM-DD), not separated into three columns.**\n",
        "\n",
        "***Make sure there are no floating point values in the date and sort the data from most recent to least.***\n",
        "\n",
        "***Remove the old columns and place the new column next to the 'id' column.***\n",
        "\n",
        "\n",
        "**YOU CAN USE MULTIPLE LINES OF CODE BUT CAN'T USE LOOPS.**\n",
        "\n",
        "**Note:** It is alright to have only a **maximum of 12 NaTs** for some dates that often go further back than the 1600s because the datetime module in Pandas has a limit (unless otherwise guided)."
      ],
      "metadata": {
        "id": "9if9utm8jsdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "new_df"
      ],
      "metadata": {
        "id": "Gs4GJqzgn2Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 2: Volcanic Matryoshkas ü™Ü (30 POINTS TOTAL)**\n",
        "\n",
        "Now, that most of the data has been tidied up. We will organize the data into more sizable pieces of information in order to extract useful information."
      ],
      "metadata": {
        "id": "IlhYym73iOZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1:** *(10 points here)*\n",
        "\n",
        "**Use the groupby function in Pandas to create separate dataframes for each unique country.**\n",
        "\n",
        "* Each table must only have the columns: 'date' 'country', 'name', and 'vei'\n",
        "\n",
        "* Sort the dataframe of each country by highest to lowest 'vei'\n",
        "\n",
        "**You MUST use the groupby function here.**"
      ],
      "metadata": {
        "id": "5yLjy9u0VuhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "YonGiaib0noL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.2:** *(5 points here)*\n",
        "\n",
        "**Using groupby again, print out the maximum 'vei' for each unique country.**\n",
        "\n",
        "**You MUST use the groupby function here.**\n",
        "\n",
        "* Print out your results in a format like the following: \"Country: {country_name}, Highest VEI: {vei}\""
      ],
      "metadata": {
        "id": "OTExPGKLA7lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "DRHCvg1b2MEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.3:** *(15 points here)*\n",
        "\n",
        "Finally, we have ALMOST REACHED THE END!!\n",
        "Since there is still quite a bit of missing data, we want to make use of what is still available.\n",
        "\n",
        "A very powerful tool in Python's magnificent collection of libraries is its beautiful graphing tools.\n",
        "\n",
        "Check out libraries such as [Seaborn](https://seaborn.pydata.org/) or [Matplotlib](https://matplotlib.org/stable/index.html) to create meaningful visualizations! **Your final task requires the use of these libraries**\n",
        "\n",
        "**Based on the unique names of volcanos, filter names that have more than 3 datapoints under their name.**\n",
        "\n",
        "* Make separate graphs for each volcano and plot their VEIs over time.\n",
        "\n",
        "* Make sure to properly label all parts of the graph."
      ],
      "metadata": {
        "id": "11rdFk2qCu8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "jvasG8eViN1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 3: Fiery Jobs üöí (15 POINTS TOTAL)**\n",
        "\n",
        "Proficiency in SQL is also super important. SQL databases are essentially relational databases in which there are vast amounts of tabular data. which can often be used to connect with related tablular data. [This](https://www.w3schools.com/sql/) is a pretty good intro into learning more about SQL."
      ],
      "metadata": {
        "id": "Gozs5JwKO7ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out this [tutorial](https://mode.com/sql-tutorial/introduction-to-sql/) for some clarifications on SQL.\n",
        "\n",
        "Now! We'll be using **sqlite** to access a database.\n",
        "* Start by downloading the sql lite file and putting it in the same directory as this [notebook](https://www.kaggle.com/datasets/kaggle/sf-salaries) (hit the 'download' button in the upper right).\n",
        "* Check out the description of the data so you know the table / column names.\n",
        "\n",
        "The following code will use sqlite3 to create a database connection. sqlite3 is the library in Python that assists in navigating through SQL databases."
      ],
      "metadata": {
        "id": "bfrjZ3wrO-R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "# import pandas as pd. Pandas was already imported from the previous sections\n",
        "\n",
        "conn = sqlite3.connect(\"database.sqlite\")\n",
        "crsr = conn.cursor()"
      ],
      "metadata": {
        "id": "ooUWEVfYPAkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code will let you check out the different tables within the database.\n",
        "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "tables = crsr.execute(query).fetchall()\n",
        "print(tables)"
      ],
      "metadata": {
        "id": "pnJiNMw7o9d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87598252-9a6a-4ea9-983c-cbe91a61eb0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Salaries',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Remember that each problem should be solved with a single sql query.**\n",
        "**All outputs must be shown**\n",
        "\n",
        "#### **3.1.1: 2 Points**\n",
        "***From the Salaries table, get the average base pay for \"firefighters\" (all job titles consisting of the word \"firefighter\" (not case sensitive)) between the year 2011 to 2013.***"
      ],
      "metadata": {
        "id": "KutbMSr2pV-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'REMOVE THIS CONTENT AND ANSWER IN HERE'\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ],
      "metadata": {
        "id": "3wm6NlYqp3dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.2: 2 Points**\n",
        "***From the Salaries table, create a table for the year 2014, with a job title of \"firefighters\" (all job titles consisting of the word \"firefighter\" (not case sensitive)) making under $100,000 as a base pay, and sort in descending order by salary.***"
      ],
      "metadata": {
        "id": "AwWzAyYbq9yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'REMOVE THIS CONTENT AND ANSWER IN HERE'\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ],
      "metadata": {
        "id": "ZQ_hK1tpq83U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.3: 4 Points**\n",
        "***Create a dataframe with averages of base pay, averages of benefits, and averages of overtime for \"firefighters\" (all job titles consisting of the word \"firefighter\" (not case sensitive)) as well as a column with the sum of these three values.***\n",
        "\n",
        "***Exclude job titles containing \"FIREFIGHTER\" (case-sensitive)***"
      ],
      "metadata": {
        "id": "zNRnMM7Cq9gG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'REMOVE THIS CONTENT AND ANSWER IN HERE'\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ],
      "metadata": {
        "id": "yVOCgk2Jq9LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.4: 7 Points**\n",
        "\n",
        "***Finally, we'll create our own table in our database. Separate the Salaries table by years, and add it back to the database. Using a loop might be helpful.***\n",
        "\n",
        "* You may use basic python to complete the task. However, using querying on SQL is **mandatory**.\n",
        "* Feel free to **use multiple lines of code for this problem only.**\n",
        "* Check out this [Hint](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html)"
      ],
      "metadata": {
        "id": "0cp88tfxq-Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS CONTENT AND ANSWER IN YOUR OWN WAY"
      ],
      "metadata": {
        "id": "YjQnj4Uqq9C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code to check if you successfully added your table.\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "print(cursor.fetchall())"
      ],
      "metadata": {
        "id": "RYcadk9g4jE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ddcf42-34b5-4dd6-8093-a133ba878c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Salaries',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![volcano](https://as1.ftcdn.net/v2/jpg/06/34/76/64/1000_F_634766457_0fZbpYj6aBLlldO1jADUPpKTRLnNmngs.jpg)"
      ],
      "metadata": {
        "id": "oW-M2Epk28g4"
      }
    }
  ]
}